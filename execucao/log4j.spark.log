17/01/27 14:09:24 INFO SparkContext: Running Spark version 1.6.2
17/01/27 14:09:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/01/27 14:09:24 INFO SecurityManager: Changing view acls to: josedejesusfilho
17/01/27 14:09:24 INFO SecurityManager: Changing modify acls to: josedejesusfilho
17/01/27 14:09:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(josedejesusfilho); users with modify permissions: Set(josedejesusfilho)
17/01/27 14:09:25 INFO Utils: Successfully started service 'sparkDriver' on port 59950.
17/01/27 14:09:25 INFO Slf4jLogger: Slf4jLogger started
17/01/27 14:09:25 INFO Remoting: Starting remoting
17/01/27 14:09:26 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@127.0.0.1:59951]
17/01/27 14:09:26 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 59951.
17/01/27 14:09:26 INFO SparkEnv: Registering MapOutputTracker
17/01/27 14:09:26 INFO SparkEnv: Registering BlockManagerMaster
17/01/27 14:09:26 INFO DiskBlockManager: Created local directory at /private/var/folders/f4/2w55zrvj5sd1kgppzrtsqcz40000gn/T/blockmgr-ef7109de-6c5b-4f93-9908-1dad5d6d6073
17/01/27 14:09:26 INFO MemoryStore: MemoryStore started with capacity 511.1 MB
17/01/27 14:09:26 INFO SparkEnv: Registering OutputCommitCoordinator
17/01/27 14:09:26 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/01/27 14:09:26 INFO SparkUI: Started SparkUI at http://127.0.0.1:4040
17/01/27 14:09:26 INFO HttpFileServer: HTTP File server directory is /private/var/folders/f4/2w55zrvj5sd1kgppzrtsqcz40000gn/T/spark-3d348d0c-9af0-4a00-b66f-6a3eb0f8cb9e/httpd-9b1f1fd0-d988-4e65-95db-dfc0f24f480c
17/01/27 14:09:26 INFO HttpServer: Starting HTTP Server
17/01/27 14:09:26 INFO Utils: Successfully started service 'HTTP file server' on port 59952.
17/01/27 14:09:26 INFO SparkContext: Added JAR file:/Users/josedejesusfilho/Library/R/3.3/library/sparklyr/java/spark-csv_2.11-1.3.0.jar at http://127.0.0.1:59952/jars/spark-csv_2.11-1.3.0.jar with timestamp 1485533366929
17/01/27 14:09:26 INFO SparkContext: Added JAR file:/Users/josedejesusfilho/Library/R/3.3/library/sparklyr/java/commons-csv-1.1.jar at http://127.0.0.1:59952/jars/commons-csv-1.1.jar with timestamp 1485533366935
17/01/27 14:09:26 INFO SparkContext: Added JAR file:/Users/josedejesusfilho/Library/R/3.3/library/sparklyr/java/univocity-parsers-1.5.1.jar at http://127.0.0.1:59952/jars/univocity-parsers-1.5.1.jar with timestamp 1485533366940
17/01/27 14:09:26 INFO SparkContext: Added JAR file:/Users/josedejesusfilho/Library/R/3.3/library/sparklyr/java/sparklyr-1.6-2.10.jar at http://127.0.0.1:59952/jars/sparklyr-1.6-2.10.jar with timestamp 1485533366945
17/01/27 14:09:27 INFO Executor: Starting executor ID driver on host localhost
17/01/27 14:09:27 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59953.
17/01/27 14:09:27 INFO NettyBlockTransferService: Server created on 59953
17/01/27 14:09:27 INFO BlockManagerMaster: Trying to register BlockManager
17/01/27 14:09:27 INFO BlockManagerMasterEndpoint: Registering block manager localhost:59953 with 511.1 MB RAM, BlockManagerId(driver, localhost, 59953)
17/01/27 14:09:27 INFO BlockManagerMaster: Registered BlockManager
17/01/27 14:09:28 INFO HiveContext: Initializing execution hive, version 1.2.1
17/01/27 14:09:28 INFO ClientWrapper: Inspected Hadoop version: 2.6.0
17/01/27 14:09:28 INFO ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0
17/01/27 14:09:29 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/01/27 14:09:29 INFO ObjectStore: ObjectStore, initialize called
17/01/27 14:09:29 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/01/27 14:09:29 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/01/27 14:09:30 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/01/27 14:09:30 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/01/27 14:09:32 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/01/27 14:09:33 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/01/27 14:09:33 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/01/27 14:09:34 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/01/27 14:09:34 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/01/27 14:09:34 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/01/27 14:09:34 INFO ObjectStore: Initialized ObjectStore
17/01/27 14:09:34 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/01/27 14:09:35 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/01/27 14:09:35 INFO HiveMetaStore: Added admin role in metastore
17/01/27 14:09:35 INFO HiveMetaStore: Added public role in metastore
17/01/27 14:09:35 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/01/27 14:09:35 INFO HiveMetaStore: 0: get_all_databases
17/01/27 14:09:35 INFO audit: ugi=josedejesusfilho	ip=unknown-ip-addr	cmd=get_all_databases	
17/01/27 14:09:35 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/01/27 14:09:35 INFO audit: ugi=josedejesusfilho	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/01/27 14:09:35 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/01/27 14:09:35 INFO SessionState: Created local directory: /var/folders/f4/2w55zrvj5sd1kgppzrtsqcz40000gn/T/fecf1101-cb6b-4b90-a5a4-cbf8d6d86bd4_resources
17/01/27 14:09:35 INFO SessionState: Created HDFS directory: /tmp/hive/josedejesusfilho/fecf1101-cb6b-4b90-a5a4-cbf8d6d86bd4
17/01/27 14:09:35 INFO SessionState: Created local directory: /var/folders/f4/2w55zrvj5sd1kgppzrtsqcz40000gn/T/josedejesusfilho/fecf1101-cb6b-4b90-a5a4-cbf8d6d86bd4
17/01/27 14:09:35 INFO SessionState: Created HDFS directory: /tmp/hive/josedejesusfilho/fecf1101-cb6b-4b90-a5a4-cbf8d6d86bd4/_tmp_space.db
17/01/27 14:09:36 INFO HiveContext: default warehouse location is /user/hive/warehouse
17/01/27 14:09:36 INFO HiveContext: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17/01/27 14:09:36 INFO ClientWrapper: Inspected Hadoop version: 2.6.0
17/01/27 14:09:36 INFO ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0
17/01/27 14:09:36 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/01/27 14:09:36 INFO ObjectStore: ObjectStore, initialize called
17/01/27 14:09:36 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/01/27 14:09:36 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/01/27 14:09:37 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/01/27 14:09:37 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/01/27 14:09:38 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/01/27 14:09:39 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/01/27 14:09:39 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/01/27 14:09:40 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/01/27 14:09:40 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/01/27 14:09:40 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/01/27 14:09:40 INFO ObjectStore: Initialized ObjectStore
17/01/27 14:09:40 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/01/27 14:09:41 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/01/27 14:09:41 INFO HiveMetaStore: Added admin role in metastore
17/01/27 14:09:41 INFO HiveMetaStore: Added public role in metastore
17/01/27 14:09:41 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/01/27 14:09:41 INFO HiveMetaStore: 0: get_all_databases
17/01/27 14:09:41 INFO audit: ugi=josedejesusfilho	ip=unknown-ip-addr	cmd=get_all_databases	
17/01/27 14:09:41 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/01/27 14:09:41 INFO audit: ugi=josedejesusfilho	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/01/27 14:09:41 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/01/27 14:09:41 INFO SessionState: Created local directory: /var/folders/f4/2w55zrvj5sd1kgppzrtsqcz40000gn/T/df39aa84-6d69-4d9e-bc3c-6572a89b4834_resources
17/01/27 14:09:41 INFO SessionState: Created HDFS directory: /tmp/hive/josedejesusfilho/df39aa84-6d69-4d9e-bc3c-6572a89b4834
17/01/27 14:09:41 INFO SessionState: Created local directory: /var/folders/f4/2w55zrvj5sd1kgppzrtsqcz40000gn/T/josedejesusfilho/df39aa84-6d69-4d9e-bc3c-6572a89b4834
17/01/27 14:09:41 INFO SessionState: Created HDFS directory: /tmp/hive/josedejesusfilho/df39aa84-6d69-4d9e-bc3c-6572a89b4834/_tmp_space.db
17/01/27 14:09:42 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
17/01/27 14:09:42 INFO audit: ugi=josedejesusfilho	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
17/01/27 14:09:43 INFO SparkContext: Starting job: collect at utils.scala:195
17/01/27 14:09:43 INFO DAGScheduler: Got job 0 (collect at utils.scala:195) with 1 output partitions
17/01/27 14:09:43 INFO DAGScheduler: Final stage: ResultStage 0 (collect at utils.scala:195)
17/01/27 14:09:43 INFO DAGScheduler: Parents of final stage: List()
17/01/27 14:09:43 INFO DAGScheduler: Missing parents: List()
17/01/27 14:09:43 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at collect at utils.scala:195), which has no missing parents
17/01/27 14:09:43 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 1968.0 B, free 1968.0 B)
17/01/27 14:09:43 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1230.0 B, free 3.1 KB)
17/01/27 14:09:43 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:59953 (size: 1230.0 B, free: 511.1 MB)
17/01/27 14:09:43 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
17/01/27 14:09:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at collect at utils.scala:195)
17/01/27 14:09:43 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
17/01/27 14:09:43 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2348 bytes)
17/01/27 14:09:43 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
17/01/27 14:09:43 INFO Executor: Fetching http://127.0.0.1:59952/jars/sparklyr-1.6-2.10.jar with timestamp 1485533366945
17/01/27 14:09:43 INFO Utils: Fetching http://127.0.0.1:59952/jars/sparklyr-1.6-2.10.jar to /private/var/folders/f4/2w55zrvj5sd1kgppzrtsqcz40000gn/T/spark-3d348d0c-9af0-4a00-b66f-6a3eb0f8cb9e/userFiles-c8dba907-4fc5-4b0b-a824-b248efda6623/fetchFileTemp6339402049069607193.tmp
17/01/27 14:09:43 INFO Executor: Adding file:/private/var/folders/f4/2w55zrvj5sd1kgppzrtsqcz40000gn/T/spark-3d348d0c-9af0-4a00-b66f-6a3eb0f8cb9e/userFiles-c8dba907-4fc5-4b0b-a824-b248efda6623/sparklyr-1.6-2.10.jar to class loader
17/01/27 14:09:43 INFO Executor: Fetching http://127.0.0.1:59952/jars/univocity-parsers-1.5.1.jar with timestamp 1485533366940
17/01/27 14:09:43 INFO Utils: Fetching http://127.0.0.1:59952/jars/univocity-parsers-1.5.1.jar to /private/var/folders/f4/2w55zrvj5sd1kgppzrtsqcz40000gn/T/spark-3d348d0c-9af0-4a00-b66f-6a3eb0f8cb9e/userFiles-c8dba907-4fc5-4b0b-a824-b248efda6623/fetchFileTemp4673242945945324359.tmp
17/01/27 14:09:43 INFO Executor: Adding file:/private/var/folders/f4/2w55zrvj5sd1kgppzrtsqcz40000gn/T/spark-3d348d0c-9af0-4a00-b66f-6a3eb0f8cb9e/userFiles-c8dba907-4fc5-4b0b-a824-b248efda6623/univocity-parsers-1.5.1.jar to class loader
17/01/27 14:09:43 INFO Executor: Fetching http://127.0.0.1:59952/jars/spark-csv_2.11-1.3.0.jar with timestamp 1485533366929
17/01/27 14:09:43 INFO Utils: Fetching http://127.0.0.1:59952/jars/spark-csv_2.11-1.3.0.jar to /private/var/folders/f4/2w55zrvj5sd1kgppzrtsqcz40000gn/T/spark-3d348d0c-9af0-4a00-b66f-6a3eb0f8cb9e/userFiles-c8dba907-4fc5-4b0b-a824-b248efda6623/fetchFileTemp3456905987162627572.tmp
17/01/27 14:09:43 INFO Executor: Adding file:/private/var/folders/f4/2w55zrvj5sd1kgppzrtsqcz40000gn/T/spark-3d348d0c-9af0-4a00-b66f-6a3eb0f8cb9e/userFiles-c8dba907-4fc5-4b0b-a824-b248efda6623/spark-csv_2.11-1.3.0.jar to class loader
17/01/27 14:09:43 INFO Executor: Fetching http://127.0.0.1:59952/jars/commons-csv-1.1.jar with timestamp 1485533366935
17/01/27 14:09:43 INFO Utils: Fetching http://127.0.0.1:59952/jars/commons-csv-1.1.jar to /private/var/folders/f4/2w55zrvj5sd1kgppzrtsqcz40000gn/T/spark-3d348d0c-9af0-4a00-b66f-6a3eb0f8cb9e/userFiles-c8dba907-4fc5-4b0b-a824-b248efda6623/fetchFileTemp531841994916408681.tmp
17/01/27 14:09:43 INFO Executor: Adding file:/private/var/folders/f4/2w55zrvj5sd1kgppzrtsqcz40000gn/T/spark-3d348d0c-9af0-4a00-b66f-6a3eb0f8cb9e/userFiles-c8dba907-4fc5-4b0b-a824-b248efda6623/commons-csv-1.1.jar to class loader
17/01/27 14:09:43 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 940 bytes result sent to driver
17/01/27 14:09:43 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 234 ms on localhost (1/1)
17/01/27 14:09:43 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/01/27 14:09:43 INFO DAGScheduler: ResultStage 0 (collect at utils.scala:195) finished in 0,259 s
17/01/27 14:09:43 INFO DAGScheduler: Job 0 finished: collect at utils.scala:195, took 0,451556 s
17/01/27 14:09:51 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
17/01/27 14:09:51 INFO audit: ugi=josedejesusfilho	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
17/01/27 14:09:51 INFO SparkContext: Starting job: collect at utils.scala:59
17/01/27 14:09:51 INFO DAGScheduler: Got job 1 (collect at utils.scala:59) with 1 output partitions
17/01/27 14:09:51 INFO DAGScheduler: Final stage: ResultStage 1 (collect at utils.scala:59)
17/01/27 14:09:51 INFO DAGScheduler: Parents of final stage: List()
17/01/27 14:09:51 INFO DAGScheduler: Missing parents: List()
17/01/27 14:09:51 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at map at utils.scala:56), which has no missing parents
17/01/27 14:09:51 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 5.4 KB, free 8.6 KB)
17/01/27 14:09:51 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.0 KB, free 11.5 KB)
17/01/27 14:09:51 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:59953 (size: 3.0 KB, free: 511.1 MB)
17/01/27 14:09:51 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
17/01/27 14:09:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at map at utils.scala:56)
17/01/27 14:09:51 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
17/01/27 14:09:51 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2348 bytes)
17/01/27 14:09:51 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
17/01/27 14:09:52 INFO GenerateUnsafeProjection: Code generated in 210.176962 ms
17/01/27 14:09:52 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1060 bytes result sent to driver
17/01/27 14:09:52 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 277 ms on localhost (1/1)
17/01/27 14:09:52 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
17/01/27 14:09:52 INFO DAGScheduler: ResultStage 1 (collect at utils.scala:59) finished in 0,279 s
17/01/27 14:09:52 INFO DAGScheduler: Job 1 finished: collect at utils.scala:59, took 0,288371 s
17/01/27 14:09:52 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 61.8 KB, free 73.3 KB)
17/01/27 14:09:52 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 19.3 KB, free 92.7 KB)
17/01/27 14:09:52 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:59953 (size: 19.3 KB, free: 511.1 MB)
17/01/27 14:09:52 INFO SparkContext: Created broadcast 2 from textFile at TextFile.scala:30
17/01/27 14:09:52 INFO FileInputFormat: Total input paths to process : 1
17/01/27 14:09:52 INFO SparkContext: Starting job: take at CsvRelation.scala:249
17/01/27 14:09:52 INFO DAGScheduler: Got job 2 (take at CsvRelation.scala:249) with 1 output partitions
17/01/27 14:09:52 INFO DAGScheduler: Final stage: ResultStage 2 (take at CsvRelation.scala:249)
17/01/27 14:09:52 INFO DAGScheduler: Parents of final stage: List()
17/01/27 14:09:52 INFO DAGScheduler: Missing parents: List()
17/01/27 14:09:52 INFO DAGScheduler: Submitting ResultStage 2 (/var/folders/f4/2w55zrvj5sd1kgppzrtsqcz40000gn/T//RtmprBsNv3/spark_serialize_d24c9c77edaec482d7b84f115b8b7b181257c6873baf636c0acaad4e6c5079d3.csv MapPartitionsRDD[7] at textFile at TextFile.scala:30), which has no missing parents
17/01/27 14:09:52 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 3.2 KB, free 95.9 KB)
17/01/27 14:09:52 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 1967.0 B, free 97.8 KB)
17/01/27 14:09:52 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:59953 (size: 1967.0 B, free: 511.1 MB)
17/01/27 14:09:52 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1006
17/01/27 14:09:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (/var/folders/f4/2w55zrvj5sd1kgppzrtsqcz40000gn/T//RtmprBsNv3/spark_serialize_d24c9c77edaec482d7b84f115b8b7b181257c6873baf636c0acaad4e6c5079d3.csv MapPartitionsRDD[7] at textFile at TextFile.scala:30)
17/01/27 14:09:52 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
17/01/27 14:09:52 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2495 bytes)
17/01/27 14:09:52 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
17/01/27 14:09:52 INFO HadoopRDD: Input split: file:/var/folders/f4/2w55zrvj5sd1kgppzrtsqcz40000gn/T/RtmprBsNv3/spark_serialize_d24c9c77edaec482d7b84f115b8b7b181257c6873baf636c0acaad4e6c5079d3.csv:0+172122
17/01/27 14:09:52 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
17/01/27 14:09:52 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
17/01/27 14:09:52 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
17/01/27 14:09:52 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
17/01/27 14:09:52 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
17/01/27 14:09:52 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2639 bytes result sent to driver
17/01/27 14:09:52 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 75 ms on localhost (1/1)
17/01/27 14:09:52 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
17/01/27 14:09:52 INFO DAGScheduler: ResultStage 2 (take at CsvRelation.scala:249) finished in 0,076 s
17/01/27 14:09:52 INFO DAGScheduler: Job 2 finished: take at CsvRelation.scala:249, took 0,101473 s
17/01/27 14:09:52 INFO ParseDriver: Parsing command: SELECT * FROM  `base`
17/01/27 14:09:54 INFO ParseDriver: Parse Completed
17/01/27 14:09:54 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 208.5 KB, free 306.3 KB)
17/01/27 14:09:54 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 19.3 KB, free 325.6 KB)
17/01/27 14:09:54 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:59953 (size: 19.3 KB, free: 511.1 MB)
17/01/27 14:09:54 INFO SparkContext: Created broadcast 4 from textFile at TextFile.scala:30
17/01/27 14:09:54 INFO FileInputFormat: Total input paths to process : 1
17/01/27 14:09:54 INFO SparkContext: Starting job: take at CsvRelation.scala:249
17/01/27 14:09:54 INFO DAGScheduler: Got job 3 (take at CsvRelation.scala:249) with 1 output partitions
17/01/27 14:09:54 INFO DAGScheduler: Final stage: ResultStage 3 (take at CsvRelation.scala:249)
17/01/27 14:09:54 INFO DAGScheduler: Parents of final stage: List()
17/01/27 14:09:54 INFO DAGScheduler: Missing parents: List()
17/01/27 14:09:54 INFO DAGScheduler: Submitting ResultStage 3 (/var/folders/f4/2w55zrvj5sd1kgppzrtsqcz40000gn/T//RtmprBsNv3/spark_serialize_d24c9c77edaec482d7b84f115b8b7b181257c6873baf636c0acaad4e6c5079d3.csv MapPartitionsRDD[9] at textFile at TextFile.scala:30), which has no missing parents
17/01/27 14:09:54 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 3.2 KB, free 328.8 KB)
17/01/27 14:09:54 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 1967.0 B, free 330.7 KB)
17/01/27 14:09:54 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:59953 (size: 1967.0 B, free: 511.1 MB)
17/01/27 14:09:54 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1006
17/01/27 14:09:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (/var/folders/f4/2w55zrvj5sd1kgppzrtsqcz40000gn/T//RtmprBsNv3/spark_serialize_d24c9c77edaec482d7b84f115b8b7b181257c6873baf636c0acaad4e6c5079d3.csv MapPartitionsRDD[9] at textFile at TextFile.scala:30)
17/01/27 14:09:54 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
17/01/27 14:09:54 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0,PROCESS_LOCAL, 2495 bytes)
17/01/27 14:09:54 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
17/01/27 14:09:54 INFO HadoopRDD: Input split: file:/var/folders/f4/2w55zrvj5sd1kgppzrtsqcz40000gn/T/RtmprBsNv3/spark_serialize_d24c9c77edaec482d7b84f115b8b7b181257c6873baf636c0acaad4e6c5079d3.csv:0+172122
17/01/27 14:09:54 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 2639 bytes result sent to driver
17/01/27 14:09:54 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 12 ms on localhost (1/1)
17/01/27 14:09:54 INFO DAGScheduler: ResultStage 3 (take at CsvRelation.scala:249) finished in 0,013 s
17/01/27 14:09:54 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
17/01/27 14:09:54 INFO DAGScheduler: Job 3 finished: take at CsvRelation.scala:249, took 0,037547 s
17/01/27 14:09:54 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 208.5 KB, free 539.3 KB)
17/01/27 14:09:54 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 19.3 KB, free 558.6 KB)
17/01/27 14:09:54 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:59953 (size: 19.3 KB, free: 511.1 MB)
17/01/27 14:09:54 INFO SparkContext: Created broadcast 6 from textFile at TextFile.scala:30
17/01/27 14:09:54 INFO FileInputFormat: Total input paths to process : 1
17/01/27 14:09:54 INFO SparkContext: Starting job: sql at NativeMethodAccessorImpl.java:-2
17/01/27 14:09:54 INFO DAGScheduler: Registering RDD 19 (sql at NativeMethodAccessorImpl.java:-2)
17/01/27 14:09:54 INFO DAGScheduler: Got job 4 (sql at NativeMethodAccessorImpl.java:-2) with 1 output partitions
17/01/27 14:09:54 INFO DAGScheduler: Final stage: ResultStage 5 (sql at NativeMethodAccessorImpl.java:-2)
17/01/27 14:09:54 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
17/01/27 14:09:54 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 4)
17/01/27 14:09:54 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[19] at sql at NativeMethodAccessorImpl.java:-2), which has no missing parents
17/01/27 14:09:54 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 19.6 KB, free 578.1 KB)
17/01/27 14:09:54 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 9.0 KB, free 587.1 KB)
17/01/27 14:09:54 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:59953 (size: 9.0 KB, free: 511.1 MB)
17/01/27 14:09:54 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1006
17/01/27 14:09:54 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[19] at sql at NativeMethodAccessorImpl.java:-2)
17/01/27 14:09:54 INFO TaskSchedulerImpl: Adding task set 4.0 with 2 tasks
17/01/27 14:09:54 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, localhost, partition 0,PROCESS_LOCAL, 2484 bytes)
17/01/27 14:09:54 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 5, localhost, partition 1,PROCESS_LOCAL, 2484 bytes)
17/01/27 14:09:54 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
17/01/27 14:09:54 INFO Executor: Running task 1.0 in stage 4.0 (TID 5)
17/01/27 14:09:54 INFO CacheManager: Partition rdd_16_1 not found, computing it
17/01/27 14:09:54 INFO CacheManager: Partition rdd_16_0 not found, computing it
17/01/27 14:09:54 INFO HadoopRDD: Input split: file:/var/folders/f4/2w55zrvj5sd1kgppzrtsqcz40000gn/T/RtmprBsNv3/spark_serialize_d24c9c77edaec482d7b84f115b8b7b181257c6873baf636c0acaad4e6c5079d3.csv:0+172122
17/01/27 14:09:54 INFO HadoopRDD: Input split: file:/var/folders/f4/2w55zrvj5sd1kgppzrtsqcz40000gn/T/RtmprBsNv3/spark_serialize_d24c9c77edaec482d7b84f115b8b7b181257c6873baf636c0acaad4e6c5079d3.csv:172122+172122
17/01/27 14:09:54 INFO GenerateUnsafeProjection: Code generated in 34.401994 ms
17/01/27 14:09:55 INFO BlockManagerInfo: Removed broadcast_5_piece0 on localhost:59953 in memory (size: 1967.0 B, free: 511.1 MB)
17/01/27 14:09:55 INFO ContextCleaner: Cleaned accumulator 6
17/01/27 14:09:55 INFO BlockManagerInfo: Removed broadcast_4_piece0 on localhost:59953 in memory (size: 19.3 KB, free: 511.1 MB)
17/01/27 14:09:55 INFO BlockManagerInfo: Removed broadcast_3_piece0 on localhost:59953 in memory (size: 1967.0 B, free: 511.1 MB)
17/01/27 14:09:55 INFO ContextCleaner: Cleaned accumulator 5
17/01/27 14:09:55 INFO BlockManagerInfo: Removed broadcast_2_piece0 on localhost:59953 in memory (size: 19.3 KB, free: 511.1 MB)
17/01/27 14:09:55 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:59953 in memory (size: 3.0 KB, free: 511.1 MB)
17/01/27 14:09:55 INFO ContextCleaner: Cleaned accumulator 4
17/01/27 14:09:55 INFO ContextCleaner: Cleaned accumulator 3
17/01/27 14:09:55 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:59953 in memory (size: 1230.0 B, free: 511.1 MB)
17/01/27 14:09:55 INFO ContextCleaner: Cleaned accumulator 1
17/01/27 14:09:55 INFO MemoryStore: Block rdd_16_0 stored as values in memory (estimated size 40.5 KB, free 296.9 KB)
17/01/27 14:09:55 INFO BlockManagerInfo: Added rdd_16_0 in memory on localhost:59953 (size: 40.5 KB, free: 511.1 MB)
17/01/27 14:09:55 INFO MemoryStore: Block rdd_16_1 stored as values in memory (estimated size 40.2 KB, free 337.1 KB)
17/01/27 14:09:55 INFO BlockManagerInfo: Added rdd_16_1 in memory on localhost:59953 (size: 40.2 KB, free: 511.0 MB)
17/01/27 14:09:55 INFO GeneratePredicate: Code generated in 11.115017 ms
17/01/27 14:09:55 INFO GenerateColumnAccessor: Code generated in 24.459709 ms
17/01/27 14:09:55 INFO GenerateMutableProjection: Code generated in 11.900802 ms
17/01/27 14:09:55 INFO GenerateUnsafeProjection: Code generated in 10.013213 ms
17/01/27 14:09:55 INFO GenerateMutableProjection: Code generated in 13.443417 ms
17/01/27 14:09:55 INFO GenerateUnsafeRowJoiner: Code generated in 12.158446 ms
17/01/27 14:09:55 INFO GenerateUnsafeProjection: Code generated in 10.425003 ms
17/01/27 14:09:55 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 3912 bytes result sent to driver
17/01/27 14:09:55 INFO Executor: Finished task 1.0 in stage 4.0 (TID 5). 3913 bytes result sent to driver
17/01/27 14:09:55 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 911 ms on localhost (1/2)
17/01/27 14:09:55 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 5) in 909 ms on localhost (2/2)
17/01/27 14:09:55 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
17/01/27 14:09:55 INFO DAGScheduler: ShuffleMapStage 4 (sql at NativeMethodAccessorImpl.java:-2) finished in 0,914 s
17/01/27 14:09:55 INFO DAGScheduler: looking for newly runnable stages
17/01/27 14:09:55 INFO DAGScheduler: running: Set()
17/01/27 14:09:55 INFO DAGScheduler: waiting: Set(ResultStage 5)
17/01/27 14:09:55 INFO DAGScheduler: failed: Set()
17/01/27 14:09:55 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at sql at NativeMethodAccessorImpl.java:-2), which has no missing parents
17/01/27 14:09:55 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 9.3 KB, free 346.4 KB)
17/01/27 14:09:55 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 4.6 KB, free 351.0 KB)
17/01/27 14:09:55 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:59953 (size: 4.6 KB, free: 511.0 MB)
17/01/27 14:09:55 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1006
17/01/27 14:09:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at sql at NativeMethodAccessorImpl.java:-2)
17/01/27 14:09:55 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks
17/01/27 14:09:55 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 6, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/27 14:09:55 INFO Executor: Running task 0.0 in stage 5.0 (TID 6)
17/01/27 14:09:55 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/27 14:09:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
17/01/27 14:09:55 INFO GenerateMutableProjection: Code generated in 9.608837 ms
17/01/27 14:09:55 INFO GenerateMutableProjection: Code generated in 8.94152 ms
17/01/27 14:09:56 INFO Executor: Finished task 0.0 in stage 5.0 (TID 6). 1830 bytes result sent to driver
17/01/27 14:09:56 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 6) in 169 ms on localhost (1/1)
17/01/27 14:09:56 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
17/01/27 14:09:56 INFO DAGScheduler: ResultStage 5 (sql at NativeMethodAccessorImpl.java:-2) finished in 0,169 s
17/01/27 14:09:56 INFO DAGScheduler: Job 4 finished: sql at NativeMethodAccessorImpl.java:-2, took 1,137355 s
17/01/27 14:09:56 INFO ParseDriver: Parsing command: SELECT count(*) FROM  `base`
17/01/27 14:09:56 INFO ParseDriver: Parse Completed
17/01/27 14:09:56 INFO SparkContext: Starting job: collect at utils.scala:195
17/01/27 14:09:56 INFO DAGScheduler: Registering RDD 26 (collect at utils.scala:195)
17/01/27 14:09:56 INFO DAGScheduler: Got job 5 (collect at utils.scala:195) with 1 output partitions
17/01/27 14:09:56 INFO DAGScheduler: Final stage: ResultStage 7 (collect at utils.scala:195)
17/01/27 14:09:56 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)
17/01/27 14:09:56 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 6)
17/01/27 14:09:56 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[26] at collect at utils.scala:195), which has no missing parents
17/01/27 14:09:56 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 19.6 KB, free 370.6 KB)
17/01/27 14:09:56 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 9.0 KB, free 379.6 KB)
17/01/27 14:09:56 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on localhost:59953 (size: 9.0 KB, free: 511.0 MB)
17/01/27 14:09:56 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1006
17/01/27 14:09:56 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[26] at collect at utils.scala:195)
17/01/27 14:09:56 INFO TaskSchedulerImpl: Adding task set 6.0 with 2 tasks
17/01/27 14:09:56 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 7, localhost, partition 0,PROCESS_LOCAL, 2484 bytes)
17/01/27 14:09:56 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 8, localhost, partition 1,PROCESS_LOCAL, 2484 bytes)
17/01/27 14:09:56 INFO Executor: Running task 0.0 in stage 6.0 (TID 7)
17/01/27 14:09:56 INFO Executor: Running task 1.0 in stage 6.0 (TID 8)
17/01/27 14:09:56 INFO BlockManager: Found block rdd_16_1 locally
17/01/27 14:09:56 INFO BlockManager: Found block rdd_16_0 locally
17/01/27 14:09:56 INFO Executor: Finished task 0.0 in stage 6.0 (TID 7). 2679 bytes result sent to driver
17/01/27 14:09:56 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 7) in 19 ms on localhost (1/2)
17/01/27 14:09:56 INFO Executor: Finished task 1.0 in stage 6.0 (TID 8). 2679 bytes result sent to driver
17/01/27 14:09:56 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 8) in 26 ms on localhost (2/2)
17/01/27 14:09:56 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
17/01/27 14:09:56 INFO DAGScheduler: ShuffleMapStage 6 (collect at utils.scala:195) finished in 0,027 s
17/01/27 14:09:56 INFO DAGScheduler: looking for newly runnable stages
17/01/27 14:09:56 INFO DAGScheduler: running: Set()
17/01/27 14:09:56 INFO DAGScheduler: waiting: Set(ResultStage 7)
17/01/27 14:09:56 INFO DAGScheduler: failed: Set()
17/01/27 14:09:56 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[29] at collect at utils.scala:195), which has no missing parents
17/01/27 14:09:56 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 9.4 KB, free 389.0 KB)
17/01/27 14:09:56 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 4.7 KB, free 393.7 KB)
17/01/27 14:09:56 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on localhost:59953 (size: 4.7 KB, free: 511.0 MB)
17/01/27 14:09:56 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1006
17/01/27 14:09:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[29] at collect at utils.scala:195)
17/01/27 14:09:56 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks
17/01/27 14:09:56 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 9, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/01/27 14:09:56 INFO Executor: Running task 0.0 in stage 7.0 (TID 9)
17/01/27 14:09:56 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/01/27 14:09:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/01/27 14:09:56 INFO Executor: Finished task 0.0 in stage 7.0 (TID 9). 1830 bytes result sent to driver
17/01/27 14:09:56 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 9) in 14 ms on localhost (1/1)
17/01/27 14:09:56 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
17/01/27 14:09:56 INFO DAGScheduler: ResultStage 7 (collect at utils.scala:195) finished in 0,014 s
17/01/27 14:09:56 INFO DAGScheduler: Job 5 finished: collect at utils.scala:195, took 0,064458 s
17/01/27 14:09:56 INFO ParseDriver: Parsing command: SELECT *
FROM `base` AS `zzz1`
WHERE (0 = 1)
17/01/27 14:09:56 INFO ParseDriver: Parse Completed
17/01/27 14:09:56 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
17/01/27 14:09:56 INFO audit: ugi=josedejesusfilho	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
17/01/27 14:09:56 INFO SparkContext: Starting job: collect at utils.scala:195
17/01/27 14:09:56 INFO DAGScheduler: Got job 6 (collect at utils.scala:195) with 1 output partitions
17/01/27 14:09:56 INFO DAGScheduler: Final stage: ResultStage 8 (collect at utils.scala:195)
17/01/27 14:09:56 INFO DAGScheduler: Parents of final stage: List()
17/01/27 14:09:56 INFO DAGScheduler: Missing parents: List()
17/01/27 14:09:56 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[31] at collect at utils.scala:195), which has no missing parents
17/01/27 14:09:56 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 1968.0 B, free 395.6 KB)
17/01/27 14:09:56 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 1225.0 B, free 396.8 KB)
17/01/27 14:09:56 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on localhost:59953 (size: 1225.0 B, free: 511.0 MB)
17/01/27 14:09:56 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1006
17/01/27 14:09:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[31] at collect at utils.scala:195)
17/01/27 14:09:56 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks
17/01/27 14:09:56 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 10, localhost, partition 0,PROCESS_LOCAL, 2646 bytes)
17/01/27 14:09:56 INFO Executor: Running task 0.0 in stage 8.0 (TID 10)
17/01/27 14:09:56 INFO Executor: Finished task 0.0 in stage 8.0 (TID 10). 1258 bytes result sent to driver
17/01/27 14:09:56 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 10) in 4 ms on localhost (1/1)
17/01/27 14:09:56 INFO DAGScheduler: ResultStage 8 (collect at utils.scala:195) finished in 0,006 s
17/01/27 14:09:56 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
17/01/27 14:09:56 INFO DAGScheduler: Job 6 finished: collect at utils.scala:195, took 0,011034 s
17/01/27 14:10:22 INFO ParseDriver: Parsing command: SELECT *
FROM `base`
17/01/27 14:10:22 INFO ParseDriver: Parse Completed
17/01/27 14:10:34 INFO ParseDriver: Parsing command: SELECT *
FROM `base`
17/01/27 14:10:34 INFO ParseDriver: Parse Completed
17/01/27 14:10:53 INFO ParseDriver: Parsing command: SELECT *
FROM `base`
17/01/27 14:10:53 INFO ParseDriver: Parse Completed
17/01/27 14:14:45 INFO SparkContext: Invoking stop() from shutdown hook
17/01/27 14:14:45 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4040
17/01/27 14:14:45 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/01/27 14:14:45 INFO MemoryStore: MemoryStore cleared
17/01/27 14:14:45 INFO BlockManager: BlockManager stopped
17/01/27 14:14:45 INFO BlockManagerMaster: BlockManagerMaster stopped
17/01/27 14:14:45 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/01/27 14:14:45 INFO SparkContext: Successfully stopped SparkContext
17/01/27 14:14:45 INFO ShutdownHookManager: Shutdown hook called
17/01/27 14:14:45 INFO ShutdownHookManager: Deleting directory /private/var/folders/f4/2w55zrvj5sd1kgppzrtsqcz40000gn/T/spark-3d348d0c-9af0-4a00-b66f-6a3eb0f8cb9e/httpd-9b1f1fd0-d988-4e65-95db-dfc0f24f480c
17/01/27 14:14:45 INFO ShutdownHookManager: Deleting directory /private/var/folders/f4/2w55zrvj5sd1kgppzrtsqcz40000gn/T/spark-3d348d0c-9af0-4a00-b66f-6a3eb0f8cb9e
17/01/27 14:14:45 INFO ShutdownHookManager: Deleting directory /private/var/folders/f4/2w55zrvj5sd1kgppzrtsqcz40000gn/T/spark-1197abcc-9c1b-4f4a-997d-ccf37a5713a2
17/01/27 14:14:45 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
17/01/27 14:14:45 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
